{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import yaml\n",
    "from baseline.utilities import *\n",
    "import copy\n",
    "\n",
    "TARGET = 'UHI Index'\n",
    "X = pd.read_parquet('../pipeline/data/processed/train/X_selected.parquet')\n",
    "y = pd.read_parquet('../pipeline/data/processed/train/y_selected.parquet')[TARGET]\n",
    "\n",
    "# 0.9738 -> 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=SEED, \n",
    "                                                    # stratify=y_cluster\n",
    "                                                    )\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{X_test.shape=}\")\n",
    "\n",
    "scaler_path = '../pipeline/models/scaler.pkl'\n",
    "with open(scaler_path, 'wb') as scaler_file:\n",
    "    pickle.dump(sc, scaler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Compare models\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoCV, ElasticNetCV\n",
    "\n",
    "models = {\n",
    "    # 'Linear Regression': LinearRegression(),\n",
    "    # 'Ridge Regression': Ridge(),\n",
    "    # 'Lasso Regression': Lasso(),\n",
    "    # 'Elastic Net': ElasticNet(),\n",
    "\n",
    "    'Random Forest': RandomForestRegressor(random_state=SEED),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=SEED),\n",
    "    'XGBoost': XGBRegressor(random_state=SEED),\n",
    "    'LightGBM': LGBMRegressor(verbosity=0, random_state=SEED),\n",
    "    'CatBoost': CatBoostRegressor(verbose=0, random_state=SEED),\n",
    "\n",
    "    'HistGradientBoosting': HistGradientBoostingRegressor(random_state=SEED),\n",
    "    'Extra Trees': ExtraTreesRegressor(random_state=SEED),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=SEED),\n",
    "    'Bagging': BaggingRegressor(random_state=SEED),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=SEED),\n",
    "\n",
    "    # 'Stacking': StackingRegressor(\n",
    "    #     estimators=[\n",
    "    #         ('et', ExtraTreesRegressor(n_jobs=-2, random_state=SEED)),\n",
    "    #         # ('rf', RandomForestRegressor(n_jobs=-2, random_state=SEED)), \n",
    "    #         # ('bg', BaggingRegressor(n_jobs=-2, random_state=SEED)),\n",
    "    #         ('xgb', XGBRegressor(n_jobs=-2, random_state=SEED)),\n",
    "    #         # ('ctb', CatBoostRegressor(verbose=0, random_state=SEED))\n",
    "    #     ],\n",
    "    #     final_estimator=ElasticNetCV(cv=10, n_jobs=-2, random_state=SEED),\n",
    "    #     verbose=2\n",
    "    # )\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"{name=}\")\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    print(f\"{name}: {mean_score:.4f} Â± {std_score:.4f}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_accuracy = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Mean CV': mean_score,\n",
    "        'Std CV': std_score,\n",
    "        'Test Accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=['Test Accuracy', 'Mean CV'], ascending=False)\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Stacking\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "rf_best_params_ = {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "xgb_best_params_ = {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 200, 'subsample': 0.7}\n",
    "extratrees_best_params_ = {'bootstrap': False, 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "elastic_net_best_params_ =  {'eps': 0.001, 'l1_ratio': 0.1, 'max_iter': 1000, 'n_alphas': 100}\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=[\n",
    "        # ('rf', RandomForestRegressor(**rf_best_params_, n_jobs=-2, random_state=SEED)), \n",
    "        ('et', ExtraTreesRegressor(**extratrees_best_params_, n_jobs=-2, random_state=SEED)),\n",
    "        ('xgb', XGBRegressor(**xgb_best_params_, n_jobs=-2, random_state=SEED)),\n",
    "        # ('ctb', CatBoostRegressor(verbose=0, random_state=SEED))\n",
    "    ],\n",
    "    final_estimator=ElasticNetCV(cv=10, n_jobs=-2, random_state=SEED),\n",
    "    verbose=2\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with open('../pipeline/models/extratrees_model.pkl', 'wb') as extratrees_file:\n",
    "    pickle.dump(model.estimators_[0], extratrees_file)\n",
    "\n",
    "with open('../pipeline/models/xgb_model.pkl', 'wb') as xgb_file:\n",
    "    pickle.dump(model.estimators_[1], xgb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Model Generalization Capability Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# * Make predictions on the training data (in-sample predictions)\n",
    "insample_predictions = model.predict(X_train)\n",
    "Y_train = y_train.tolist()\n",
    "print(f\"{r2_score(Y_train, insample_predictions)=}\")\n",
    "\n",
    "# * Make predictions on the test data (out-sample predictions)\n",
    "outsample_predictions = model.predict(X_test)\n",
    "Y_test = y_test.tolist()\n",
    "print(f\"{r2_score(Y_test, outsample_predictions)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Save the model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "os.makedirs('../pipeline/models', exist_ok=True)\n",
    "model_path = '../pipeline/models/stacking.pkl'\n",
    "\n",
    "with open(model_path, 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
