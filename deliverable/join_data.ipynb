{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from baseline.utilities import *\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=False, nb_workers=8)\n",
    "\n",
    "# -------------------------------\n",
    "# INITIAL CONFIGURATION\n",
    "# -------------------------------\n",
    "SENTINEL_TIFF_PATH = '../baseline/S2_sample.tiff' # './S2_sample_5res.tiff'\n",
    "LANDSAT_TIFF_PATH = '../baseline/Landsat_LST.tiff'\n",
    "MODE = 'train'  # 'submission' 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# DATA LOADING\n",
    "# -------------------------------\n",
    "\n",
    "if MODE == 'train':\n",
    "    ground_df = pd.read_csv(\"../../baseline/Training_data_uhi_index.csv\")\n",
    "elif MODE == 'submission':\n",
    "    ground_df = pd.read_csv(\"../../baseline/Submission_template.csv\")\n",
    "else:\n",
    "    raise ValueError(\"MODE should be either 'train' or 'submission\")\n",
    "\n",
    "sentinel_bands_df = pd.read_parquet(f'../data/processed/{MODE}/sentinel2_bands.parquet')\n",
    "print(f\"{sentinel_bands_df.columns=}\")\n",
    "sentinel_features_df = pd.read_parquet(f'../data/processed/{MODE}/sentinel2_focal_buffers.parquet')\n",
    "print(f\"{sentinel_features_df.columns=}\")\n",
    "sentinel_features_bands_df = pd.read_parquet(f'../data/processed/{MODE}/sentinel2_focal_buffers_bands.parquet')\n",
    "print(f\"{sentinel_features_bands_df.columns=}\")\n",
    "landsat_features_df = pd.read_parquet(f'../data/processed/{MODE}/landsat.parquet')\n",
    "print(f\"{landsat_features_df.columns=}\")\n",
    "bldng_footprint = pd.read_parquet(f'../data/processed/{MODE}/building_footprint.parquet')\n",
    "print(f\"{bldng_footprint.columns=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# * Joining the predictor variables and response variables\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Combining ground data, focal radius data and satellite bands data into a single dataset.\n",
    "uhi_data = combine_two_datasets(ground_df,sentinel_bands_df)\n",
    "uhi_data = combine_two_datasets(uhi_data, sentinel_features_df)\n",
    "# uhi_data = combine_two_datasets(uhi_data, sentinel_pct_change_df)\n",
    "uhi_data = combine_two_datasets(uhi_data, landsat_features_df)\n",
    "uhi_data = combine_two_datasets(uhi_data, sentinel_features_bands_df)\n",
    "# uhi_data = combine_two_datasets(uhi_data, ny_mesonet_features_df)\n",
    "uhi_data = combine_two_datasets(uhi_data, bldng_footprint)\n",
    "\n",
    "print(f\"{uhi_data.columns=}\")\n",
    "\n",
    "all_features = uhi_data.copy()\n",
    "for col in all_features.columns:\n",
    "    print(col)\n",
    "\n",
    "# Remove duplicate rows from the DataFrame based on specified columns and keep the first occurrence\n",
    "columns_to_check = \\\n",
    "    ['B01', 'B06', 'UHI Index', 'B02', 'B03', 'B04', 'B05', 'B07', 'B08', 'B8A', 'B11', 'B12'] # + \\\n",
    "    # ['NDVI', 'gNDBI', 'UI', 'NDBI', 'NBI', 'BRBA', 'NBAI', 'MBI', 'BAEI', 'gCI']\n",
    "\n",
    "for col in columns_to_check:\n",
    "    # Check if the value is a numpy array and has more than one dimension\n",
    "    uhi_data[col] = uhi_data[col].apply(lambda x: tuple(x) if isinstance(x, np.ndarray) and x.ndim > 0 else x)\n",
    "\n",
    "# Now remove duplicates\n",
    "if MODE == 'train':\n",
    "    uhi_data.to_parquet(f'../data/processed/{MODE}/train_data.parquet')\n",
    "    uhi_data = uhi_data.drop_duplicates(subset=columns_to_check, keep='first')\n",
    "\n",
    "# Resetting the index of the dataset\n",
    "uhi_data = uhi_data.reset_index(drop=True)\n",
    "print(f\"{uhi_data.shape=}\")\n",
    "print(uhi_data.isna().sum())\n",
    "\n",
    "# Saving the dataset to a parquet file\n",
    "if MODE == 'train':\n",
    "    uhi_data.to_parquet(f'../data/processed/{MODE}/train_data.parquet')\n",
    "elif MODE == 'submission':\n",
    "    uhi_data.to_parquet(f'../data/processed/{MODE}/submission_data.parquet')\n",
    "else:\n",
    "    raise ValueError(\"MODE should be either 'train' or 'submission'\")\n",
    "\n",
    "print(f\"{uhi_data.shape=}\")\n",
    "\n",
    "with open('../data/columns.json', mode='w') as f:\n",
    "    json.dump({'features': [c for c in uhi_data.columns if c not in ['Longitude', 'Latitude', 'UHI Index', 'datetime']]}, f, indent=4)\n",
    "\n",
    "print(f\"{list(uhi_data.columns)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# DATA SAVING\n",
    "# -------------------------------\n",
    "\n",
    "import yaml\n",
    "with open('../data/columns.yml', mode='w') as f:\n",
    "    yaml.dump({'features': [c for c in uhi_data.columns if c not in ['Longitude', 'Latitude', 'UHI Index', 'datetime']]}, f)\n",
    "\n",
    "# Open Yaml\n",
    "feature_list = yaml.safe_load(open('../data/columns.yml', 'r'))['features']\n",
    "print(len(feature_list))\n",
    "print(feature_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
